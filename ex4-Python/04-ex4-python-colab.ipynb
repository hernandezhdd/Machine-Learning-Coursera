{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hernandezhdd/Machine-Learning-Stanford/blob/main/ex4-Python/04-ex4-python-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4 Python: Regularized Linear Regression and Bias-Variance"
      ],
      "metadata": {
        "id": "1QcLrYgCz0j1"
      },
      "id": "1QcLrYgCz0j1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download files from Exercise 4 GitHub folder\n",
        "\n",
        "Run the next two cells to download the necessary function files, scripts and datasets."
      ],
      "metadata": {
        "id": "ZCSutR1__xR5"
      },
      "id": "ZCSutR1__xR5"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install subversion"
      ],
      "metadata": {
        "id": "Vl06tfxJ9MYH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Vl06tfxJ9MYH"
    },
    {
      "cell_type": "code",
      "source": [
        "!svn checkout https://github.com/hernandezhdd/Machine-Learning-Stanford/trunk/ex4_py"
      ],
      "metadata": {
        "id": "gEgzQ6bj7ef0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gEgzQ6bj7ef0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move files to /content folder"
      ],
      "metadata": {
        "id": "xA8XFnAv_3mo"
      },
      "id": "xA8XFnAv_3mo"
    },
    {
      "cell_type": "code",
      "source": [
        "!mv  -v ../content/ex4_py/* ../content/"
      ],
      "metadata": {
        "id": "qtsfyLzW9LOz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qtsfyLzW9LOz"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dcd8d582-c668-46e5-b396-da0c145a5ae4",
      "metadata": {
        "id": "dcd8d582-c668-46e5-b396-da0c145a5ae4"
      },
      "outputs": [],
      "source": [
        "# %% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
        "\n",
        "# %  Instructions\n",
        "# %  ------------\n",
        "# % \n",
        "# %  This file contains code that helps you get started on the\n",
        "# %  linear exercise. You will need to complete the following functions \n",
        "# %  in this exericse:\n",
        "# %\n",
        "# %     sigmoidGradient.m\n",
        "# %     randInitializeWeights.m\n",
        "# %     nnCostFunction.m\n",
        "# %\n",
        "# %  For this exercise, you will not need to change any code in this file,\n",
        "# %  or any other files other than those mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "72dabc1e-8066-44b3-a2c8-622859aa21ef",
      "metadata": {
        "id": "72dabc1e-8066-44b3-a2c8-622859aa21ef",
        "outputId": "10d7280a-4f56-45b0-a52e-a3d00e155fe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and Visualizing Data ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Loading and Visualizing Data ...\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "7a1f7b0a-501f-4477-92e9-0c0cd7f7bfe3",
      "metadata": {
        "id": "7a1f7b0a-501f-4477-92e9-0c0cd7f7bfe3",
        "outputId": "47b58cd0-1c28-4b06-9d47-625101af1d73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and Visualizing Data ...\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import scipy.io as scio\n",
        "\n",
        "# %% Setup the parameters you will use for this exercise\n",
        "input_layer_size  = 400  #% 20x20 Input Images of Digits\n",
        "hidden_layer_size = 25   #% 25 hidden units\n",
        "num_labels = 10          #% 10 labels, from 1 to 10   \n",
        "                          #% (note that we have mapped \"0\" to label 10)\n",
        "\n",
        "# %% =========== Part 1: Loading and Visualizing Data =============\n",
        "# %  We start the exercise by first loading and visualizing the dataset. \n",
        "# %  You will be working with a dataset that contains handwritten digits.\n",
        "# %\n",
        "\n",
        "# % Load Training Data\n",
        "print('Loading and Visualizing Data ...\\n')\n",
        "\n",
        "ex4data1 = scio.loadmat('ex4data1.mat')\n",
        "\n",
        "X = ex4data1['X']\n",
        "y = ex4data1['y']\n",
        "\n",
        "m = X.shape[0]\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "KwCcLzHOREWl",
        "outputId": "d319f965-d311-43b2-b523-e47519b6eb76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KwCcLzHOREWl",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ae1ebcc8-a6cb-40e7-a5e6-9c16b4622ecb",
      "metadata": {
        "id": "ae1ebcc8-a6cb-40e7-a5e6-9c16b4622ecb",
        "outputId": "fe80d3ee-3562-429f-c0a5-abb2c8cd5baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# % Randomly select 100 data points to display\n",
        "# sel = randperm(size(X, 1));\n",
        "# sel = sel[1:100];\n",
        "\n",
        "# displayData(X[sel, :]);\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(-X[3500,:].reshape(20,20).T, cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9d6b5f60-4227-449a-9955-e40f1a9132f1",
      "metadata": {
        "id": "9d6b5f60-4227-449a-9955-e40f1a9132f1",
        "outputId": "03190e45-8115-4100-8ec4-90d8b2cc6b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Saved Neural Network Parameters ...\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10285, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# %% ================ Part 2: Loading Parameters ================\n",
        "# % In this part of the exercise, we load some pre-initialized \n",
        "# % neural network parameters.\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "print('\\nLoading Saved Neural Network Parameters ...\\n')\n",
        "\n",
        "# % Load the weights into variables Theta1 and Theta2\n",
        "\n",
        "ex4weights = scipy.io.loadmat('ex4weights.mat')\n",
        "\n",
        "Theta1 = ex4weights['Theta1']\n",
        "Theta2 = ex4weights['Theta2']\n",
        "\n",
        "# load('ex4weights.mat');\n",
        "\n",
        "# % Unroll parameters \n",
        "nn_params = np.append(Theta1 , Theta2)\n",
        "nn_params = nn_params.reshape(len(nn_params),1)\n",
        "nn_params.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6aede2ba-9d4e-4ae4-99a0-783c02702397",
      "metadata": {
        "id": "6aede2ba-9d4e-4ae4-99a0-783c02702397"
      },
      "outputs": [],
      "source": [
        "def nnCostFunction(nn_params, input_lyr_sz, hidd_lyr_sz, num_lbls, X, y, lmbd):\n",
        "                                   \n",
        "    # %NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
        "    # %neural network which performs classification\n",
        "    # %   [J grad] = NNCOSTFUNCTON(nn_params, hidd_lyr_sz, num_lbls, ...\n",
        "    # %   X, y, lmbd) computes the cost and gradient of the neural network. The\n",
        "    # %   parameters for the neural network are \"unrolled\" into the vector\n",
        "    # %   nn_params and need to be converted back into the weight matrices. \n",
        "    # % \n",
        "    # %   The returned parameter grad should be a \"unrolled\" vector of the\n",
        "    # %   partial derivatives of the neural network.\n",
        "    # %\n",
        "    # % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
        "    # % for our 2 layer neural network\n",
        "    \n",
        "    import numpy as np\n",
        "    from sigmoid import sigmoid\n",
        "    \n",
        "    Theta1 = np.reshape(nn_params[0:hidd_lyr_sz * (input_lyr_sz + 1)], (hidd_lyr_sz, input_lyr_sz + 1))\n",
        "\n",
        "    Theta2 = np.reshape(nn_params[hidd_lyr_sz * (input_lyr_sz + 1):], (num_lbls, hidd_lyr_sz + 1))\n",
        "\n",
        "    # % Setup some useful variables\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # % You need to return the following variables correctly \n",
        "\n",
        "    J = 0\n",
        "    Theta1_grad = np.zeros(Theta1.shape)\n",
        "    Theta2_grad = np.zeros(Theta2.shape)\n",
        "\n",
        "    # % Add ones to the X data matrix\n",
        "    \n",
        "    X = np.append(np.ones((m,1)), X, axis=1)\n",
        "    \n",
        "    # tic #QUE HAGO CON ESTO\n",
        "\n",
        "    # % ====================== YOUR CODE HERE ======================\n",
        "    # % Instructions: You should complete the code by working through the\n",
        "    # %               following parts.\n",
        "    # %\n",
        "    # % Part 1: Feedforward the neural network and return the cost in the\n",
        "    # %         variable J. After implementing Part 1, you can verify that your\n",
        "    # %         cost function computation is correct by verifying the cost\n",
        "    # %         computed in ex4.m\n",
        "    # %\n",
        "\n",
        "    yVec = np.zeros (( len(y), num_lbls))\n",
        "    y = y.reshape((len(y),1))\n",
        "\n",
        "    for k in range(0,num_lbls):\n",
        "        yVec[:,k] = np.where( y==k+1, 1,0 ).reshape((len(y)))\n",
        "        \n",
        "    # Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
        "    # a2 5000X25 a3 \n",
        "\n",
        "    z2 = np.dot(X, Theta1.T)\n",
        "\n",
        "    a2 = sigmoid( z2)\n",
        "    a2 = np.append(np.ones((a2.shape[0],1)), a2, axis=1) # a2 5000X26 \n",
        "    \n",
        "    #h_theta\n",
        "    z3 = np.dot(a2, Theta2.T)\n",
        "\n",
        "    a3 = sigmoid( z3); # a3 5000X10\n",
        "\n",
        "    J = (1- yVec) * np.log (1 - a3)\n",
        "\n",
        "    J = J + yVec * np.log (a3)\n",
        "\n",
        "    J = - 1/m * np.sum (np.sum( J ))\n",
        "\n",
        "    J = J + 0.5*lmbd/m*  np.sum( np.sum( Theta1[:,1:]**2)) \n",
        "\n",
        "    J = J + 0.5*lmbd/m* np.sum( np.sum( Theta2[:,1:]**2))\n",
        "\n",
        "    # % Part 2: Implement the backpropagation algorithm to compute the gradients\n",
        "    # %         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
        "    # %         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
        "    # %         Theta2_grad, respectively. After implementing Part 2, you can check\n",
        "    # %         that your implementation is correct by running checkNNGradients\n",
        "    # %\n",
        "    # %         Note: The vector y passed into the function is a vector of labels\n",
        "    # %               containing values from 1..K. You need to map this vector into a \n",
        "    # %               binary vector of 1's and 0's to be used with the neural network\n",
        "    # %               cost function.\n",
        "    # %\n",
        "    # %         Hint: We recommend implementing backpropagation using a for-loop\n",
        "    # %               over the training examples if you are implementing it for the \n",
        "    # %               first time.\n",
        "    # %\n",
        "\n",
        "    # Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
        "    # a2 5000X25 a3 5000X10\n",
        "\n",
        "    Delta1 = np.zeros( (hidd_lyr_sz, input_lyr_sz+1) )\n",
        "\n",
        "    Delta2 = np.zeros( (num_lbls, hidd_lyr_sz+1 ))\n",
        "\n",
        "    delta3 = a3 - yVec\n",
        "    \n",
        "    # delta2 = (  (Theta2' * delta3')' .* ( (a2.*( 1 - a2 )) )) ;\n",
        "#     ##           26X10      10X5000          5000X26\n",
        "    delta2 = (  np.dot(Theta2.T, delta3.T) ).T\n",
        "    \n",
        "    delta2 = delta2* ( a2*( 1 - a2))\n",
        "    \n",
        "    Delta2 = Delta2 + np.dot(delta3.T, a2)\n",
        "    \n",
        "    delta1 = np.dot(Theta1.T, delta2[:,1:].T).T\n",
        "    \n",
        "    delta1 =  delta1 * ( X* ( 1-X ) )\n",
        "    \n",
        "    delta1 = np.sum( delta1)\n",
        "    \n",
        "    Delta1 = Delta1 + np.dot(delta2.T, X)[1:,:];  \n",
        "    \n",
        "    Theta1_grad = np.zeros( (hidd_lyr_sz, input_lyr_sz+1) )\n",
        "\n",
        "    Theta2_grad = np.zeros( (num_lbls, hidd_lyr_sz+1 ))\n",
        "    \n",
        "    Theta1_grad[:, 1:] = Delta1[:, 1:] / m;\n",
        "\n",
        "    Theta2_grad[:, 1:] = Delta2[:, 1:] / m;\n",
        "\n",
        "#     % Part 3: Implement regularization with the cost function and gradients.\n",
        "#     %\n",
        "#     %         Hint: You can implement this around the code for\n",
        "#     %               backpropagation. That is, you can compute the gradients for\n",
        "#     %               the regularization separately and then add them to Theta1_grad\n",
        "#     %               and Theta2_grad from Part 2.\n",
        "#     %\n",
        "#     Theta1_grad(:,2:end) = Theta1_grad(:,2:end) + lmbd/m * Theta1(:,2:end);\n",
        "\n",
        "#     Theta2_grad(:,2:end) = Theta2_grad(:,2:end) + lmbd/m * Theta2(:,2:end);\n",
        "\n",
        "#     % -------------------------------------------------------------\n",
        "\n",
        "#     % =========================================================================\n",
        "\n",
        "#     % Unroll gradients\n",
        "#     grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
        "\n",
        "#     tiempos=toc;\n",
        "\n",
        "    # return [J, grad, tiempos]\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5fdb3a07-e7b7-4be0-b80a-55e4f44b7774",
      "metadata": {
        "id": "5fdb3a07-e7b7-4be0-b80a-55e4f44b7774",
        "outputId": "7970978b-4d17-4260-bbaf-3669c3756b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feedforward Using Neural Network ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %% ================ Part 3: Compute Cost (Feedforward) ================\n",
        "# %  To the neural network, you should first start by implementing the\n",
        "# %  feedforward part of the neural network that returns the cost only. You\n",
        "# %  should complete the code in nnCostFunction.m to return cost. After\n",
        "# %  implementing the feedforward to compute the cost, you can verify that\n",
        "# %  your implementation is correct by verifying that you get the same cost\n",
        "# %  as us for the fixed debugging parameters.\n",
        "# %\n",
        "# %  We suggest implementing the feedforward cost *without* regularization\n",
        "# %  first so that it will be easier for you to debug. Later, in part 4, you\n",
        "# %  will get to implement the regularized cost.\n",
        "# %\n",
        "print('\\nFeedforward Using Neural Network ...\\n')\n",
        "\n",
        "# % Weight regularization parameter (we set this to 0 here).\n",
        "lmbd = 0;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "48b59690-721b-42e2-b153-f8e359280197",
      "metadata": {
        "id": "48b59690-721b-42e2-b153-f8e359280197",
        "outputId": "cb7a5a54-85dd-40d1-bdd7-d961496a138f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at parameters (loaded from ex4weights): 0.2876291651613189 \n",
            "(this value should be about 0.287629)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# from nnCostFunction import nnCostFunction\n",
        "\n",
        "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd)\n",
        "\n",
        "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.287629)\\n');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "99e8a429-0e9a-46e0-8817-a29e9217c3ed",
      "metadata": {
        "id": "99e8a429-0e9a-46e0-8817-a29e9217c3ed"
      },
      "outputs": [],
      "source": [
        "# Time taken to calculate cost function and gradients using unvectorized function\n",
        "# tiempos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4644f581-0cfa-4c38-a91a-a12d1d1f012c",
      "metadata": {
        "id": "4644f581-0cfa-4c38-a91a-a12d1d1f012c",
        "outputId": "ce4a593e-49f2-44ad-8f3f-db2ffc9828ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking Cost Function (w/ Regularization) ... \n",
            "\n",
            "Cost at parameters (loaded from ex4weights): 0.38376985909092365 \n",
            "(this value should be about 0.383770)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %% =============== Part 4: Implement Regularization ===============\n",
        "# %  Once your cost function implementation is correct, you should now\n",
        "# %  continue to implement the regularization with the cost.\n",
        "# %\n",
        "\n",
        "print('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
        "\n",
        "# % Weight regularization parameter (we set this to 1 here).\n",
        "lmbd = 1;\n",
        "\n",
        "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd);\n",
        "\n",
        "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.383770)\\n');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0d0d717d-dee6-4c26-ab25-86330e61b69c",
      "metadata": {
        "id": "0d0d717d-dee6-4c26-ab25-86330e61b69c"
      },
      "outputs": [],
      "source": [
        "# %% ================ Part 5: Sigmoid Gradient  ================\n",
        "# %  Before you start implementing the neural network, you will first\n",
        "# %  implement the gradient for the sigmoid function. You should complete the\n",
        "# %  code in the sigmoidGradient.m file.\n",
        "# %\n",
        "\n",
        "def sigmoidGradient(z):\n",
        "# %SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
        "# %evaluated at z\n",
        "# %   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
        "# %   evaluated at z. This should work regardless if z is a matrix or a\n",
        "# %   vector. In particular, if z is a vector or matrix, you should return\n",
        "# %   the gradient for each element.\n",
        "    \n",
        "    import numpy as np\n",
        "    from sigmoid import sigmoid\n",
        "    \n",
        "    z = np.array(z)\n",
        "    \n",
        "    g = np.zeros(z.shape)\n",
        "\n",
        "# % ====================== YOUR CODE HERE ======================\n",
        "# % Instructions: Compute the gradient of the sigmoid function evaluated at\n",
        "# %               each value of z (z can be a matrix, vector or scalar).\n",
        "\n",
        "    g = sigmoid(z) * (1 - sigmoid(z));\n",
        "    return g\n",
        "\n",
        "# % ============================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7c464553-8465-4a33-a905-55f189c4e1f7",
      "metadata": {
        "id": "7c464553-8465-4a33-a905-55f189c4e1f7",
        "outputId": "238e0977-babe-4749-c01c-503de6d6f9ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating sigmoid gradient...\n",
            "\n",
            "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
            "  \n",
            "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
          ]
        }
      ],
      "source": [
        "print('\\nEvaluating sigmoid gradient...\\n')\n",
        "\n",
        "g = sigmoidGradient([-1, -0.5, 0, 0.5, 1])\n",
        "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "41fec75d-e666-414d-bb16-e45ee897ed19",
      "metadata": {
        "id": "41fec75d-e666-414d-bb16-e45ee897ed19"
      },
      "outputs": [],
      "source": [
        "# %% ================ Part 6: Initializing Pameters ================\n",
        "# %  In this part of the exercise, you will be starting to implment a two\n",
        "# %  layer neural network that classifies digits. You will start by\n",
        "# %  implementing a function to initialize the weights of the neural network\n",
        "# %  (randInitializeWeights.m)\n",
        "\n",
        "def randInitializeWeights(L_in, L_out):\n",
        "    # %RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
        "    # %incoming connections and L_out outgoing connections\n",
        "    # %   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
        "    # %   of a layer with L_in incoming connections and L_out outgoing \n",
        "    # %   connections. \n",
        "    # %\n",
        "    # %   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
        "    # %   the first column of W handles the \"bias\" terms\n",
        "    # %\n",
        "\n",
        "    # % You need to return the following variables correctly \n",
        "    W = np.zeros((L_out, 1 + L_in))\n",
        "\n",
        "    # % ====================== YOUR CODE HERE ======================\n",
        "    # % Instructions: Initialize W randomly so that we break the symmetry while\n",
        "    # %               training the neural network.\n",
        "    # %\n",
        "    # % Note: The first column of W corresponds to the parameters for the bias unit\n",
        "    # %\n",
        "\n",
        "    # % Randomly initialize the weights to small values\n",
        "    ##epsilon init = 0.12;\n",
        "    \n",
        "    epsilon_init = np.sqrt(6)/np.sqrt(L_in + L_out)\n",
        "\n",
        "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
        "\n",
        "    return W\n",
        "\n",
        "# % ========================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "17518287-2f85-4d1b-bf9d-2ef0cf9f0a2e",
      "metadata": {
        "id": "17518287-2f85-4d1b-bf9d-2ef0cf9f0a2e",
        "outputId": "230feba1-08ad-44ec-d7f2-ca92cd94a10e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Neural Network Parameters ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('\\nInitializing Neural Network Parameters ...\\n')\n",
        "\n",
        "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
        "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
        "\n",
        "# % Unroll parameters\n",
        "initial_nn_params = np.append(initial_Theta1 , initial_Theta2)\n",
        "initial_nn_params = initial_nn_params.reshape(len(nn_params),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c44b1499-af01-4889-aa76-9bcb93e1f114",
      "metadata": {
        "id": "c44b1499-af01-4889-aa76-9bcb93e1f114",
        "outputId": "56ac6dae-8430-4e23-bf3d-6dfd556b0352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking Backpropagation... \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %% =============== Part 7: Implement Backpropagation ===============\n",
        "# %  Once your cost matches up with ours, you should proceed to implement the\n",
        "# %  backpropagation algorithm for the neural network. You should add to the\n",
        "# %  code you've written in nnCostFunction.m to return the partial\n",
        "# %  derivatives of the parameters.\n",
        "# %\n",
        "print('\\nChecking Backpropagation... \\n');\n",
        "\n",
        "# %  Check gradients by running checkNNGradients\n",
        "# checkNNGradients;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "877f1a5d-0615-42ab-b81e-aea983096333",
      "metadata": {
        "id": "877f1a5d-0615-42ab-b81e-aea983096333",
        "outputId": "f87f2147-ff28-4df6-f358-efd1de7a416f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Saved Neural Network Parameters ...\n",
            "\n",
            "Loading and Visualizing Data ...\n",
            "\n",
            "(5000, 10) (5000, 10)\n"
          ]
        }
      ],
      "source": [
        "# %% ================ Part 2: Loading Parameters ================\n",
        "# % In this part of the exercise, we load some pre-initialized \n",
        "# % neural network parameters.\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "print('\\nLoading Saved Neural Network Parameters ...\\n')\n",
        "\n",
        "# % Load the weights into variables Theta1 and Theta2\n",
        "\n",
        "ex4weights = scipy.io.loadmat('ex4weights.mat')\n",
        "\n",
        "Theta1 = ex4weights['Theta1']\n",
        "Theta2 = ex4weights['Theta2']\n",
        "\n",
        "# load('ex4weights.mat');\n",
        "\n",
        "# % Unroll parameters \n",
        "nn_params = np.append(Theta1 , Theta2)\n",
        "nn_params = nn_params.reshape(len(nn_params),1)\n",
        "nn_params.shape\n",
        "\n",
        "\n",
        "\n",
        "nn_params, input_lyr_sz, hidd_lyr_sz, num_lbls, X, y, lmbd = nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd\n",
        "\n",
        "import scipy.io as scio\n",
        "\n",
        "# %% Setup the parameters you will use for this exercise\n",
        "input_layer_size  = 400  #% 20x20 Input Images of Digits\n",
        "hidden_layer_size = 25   #% 25 hidden units\n",
        "num_labels = 10          #% 10 labels, from 1 to 10   \n",
        "                          #% (note that we have mapped \"0\" to label 10)\n",
        "\n",
        "# %% =========== Part 1: Loading and Visualizing Data =============\n",
        "# %  We start the exercise by first loading and visualizing the dataset. \n",
        "# %  You will be working with a dataset that contains handwritten digits.\n",
        "# %\n",
        "\n",
        "# % Load Training Data\n",
        "print('Loading and Visualizing Data ...\\n')\n",
        "\n",
        "ex4data1 = scio.loadmat('ex4data1.mat')\n",
        "\n",
        "X = ex4data1['X']\n",
        "y = ex4data1['y']\n",
        "\n",
        "m = X.shape[0]\n",
        "\n",
        "# %NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
        "# %neural network which performs classification\n",
        "# %   [J grad] = NNCOSTFUNCTON(nn_params, hidd_lyr_sz, num_lbls, ...\n",
        "# %   X, y, lmbd) computes the cost and gradient of the neural network. The\n",
        "# %   parameters for the neural network are \"unrolled\" into the vector\n",
        "# %   nn_params and need to be converted back into the weight matrices. \n",
        "# % \n",
        "# %   The returned parameter grad should be a \"unrolled\" vector of the\n",
        "# %   partial derivatives of the neural network.\n",
        "# %\n",
        "# % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
        "# % for our 2 layer neural network\n",
        "\n",
        "import numpy as np\n",
        "    \n",
        "Theta1 = np.reshape(nn_params[0:hidd_lyr_sz * (input_lyr_sz + 1)], (hidd_lyr_sz, input_lyr_sz + 1))\n",
        "\n",
        "Theta2 = np.reshape(nn_params[hidd_lyr_sz * (input_lyr_sz + 1):], (num_lbls, hidd_lyr_sz + 1))\n",
        "\n",
        "# % Setup some useful variables\n",
        "m = X.shape[0]\n",
        "\n",
        "# % You need to return the following variables correctly \n",
        "\n",
        "J = 0\n",
        "Theta1_grad = np.zeros(Theta1.shape)\n",
        "Theta2_grad = np.zeros(Theta2.shape)\n",
        "\n",
        "# % Add ones to the X data matrix\n",
        "\n",
        "X = np.append(np.ones((m,1)), X, axis=1)\n",
        "\n",
        "# tic #QUE HAGO CON ESTO\n",
        "\n",
        "# % ====================== YOUR CODE HERE ======================\n",
        "# % Instructions: You should complete the code by working through the\n",
        "# %               following parts.\n",
        "# %\n",
        "# % Part 1: Feedforward the neural network and return the cost in the\n",
        "# %         variable J. After implementing Part 1, you can verify that your\n",
        "# %         cost function computation is correct by verifying the cost\n",
        "# %         computed in ex4.m\n",
        "# %\n",
        "\n",
        "# labelsVec = np.append(np.zeros((k,1)), 1)\n",
        "# labelsVec = np.append(labelsVec, np.zeros((num_lbls-k-1, 1)))\n",
        "# labelsVec = labelsVec.T\n",
        "\n",
        "import numpy.matlib\n",
        "\n",
        "# yVec = np.zeros (( len(y), 1, num_lbls+1))\n",
        "\n",
        "yVec = np.matlib.repmat(y, 1,num_lbls)\n",
        "\n",
        "for k in range(1,num_lbls+1):\n",
        "    yVec[:,k-1] = np.where( yVec[:,k-1]==k, 1,0 )\n",
        "\n",
        "    # idxs = np.find( y==k )\n",
        "\n",
        "    # WTFFF\n",
        "    # yVec[ idxs, : ] = repmat(labelsVec, length(idxs), 1)\n",
        "\n",
        "# Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
        "# a2 5000X25 a3 \n",
        "\n",
        "z2 = np.dot(X, Theta1.T)\n",
        "\n",
        "from sigmoid import sigmoid\n",
        "\n",
        "a2 = sigmoid( z2)\n",
        "# [np.ones(m, 1) X]\n",
        "a2 = np.append(np.ones((a2.shape[0],1)), a2, axis=1)\n",
        "# a2 = [np.ones(a2.shape[0], 1) a2]; # a2 5000X26 \n",
        "\n",
        "#h_theta\n",
        "z3 = np.dot(a2, Theta2.T)\n",
        "\n",
        "# global a3\n",
        "\n",
        "a3 = sigmoid( z3); # a3 5000X10\n",
        "\n",
        "print(a3.shape, yVec.shape)\n",
        "\n",
        "J = (1- yVec) * np.log (1 - a3)\n",
        "\n",
        "J = J + yVec * np.log (a3)\n",
        "\n",
        "J = - 1/m * np.sum (np.sum( J ))\n",
        "\n",
        "J = J + 0.5*lmbd/m*  np.sum( np.sum( Theta1[:,1:]**2)) \n",
        "\n",
        "J = J + np.sum( np.sum( Theta2[:,1:]**2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "name": "ex4_py.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

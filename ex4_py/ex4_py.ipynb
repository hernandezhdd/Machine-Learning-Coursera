{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8d582-c668-46e5-b396-da0c145a5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
    "\n",
    "# %  Instructions\n",
    "# %  ------------\n",
    "# % \n",
    "# %  This file contains code that helps you get started on the\n",
    "# %  linear exercise. You will need to complete the following functions \n",
    "# %  in this exericse:\n",
    "# %\n",
    "# %     sigmoidGradient.m\n",
    "# %     randInitializeWeights.m\n",
    "# %     nnCostFunction.m\n",
    "# %\n",
    "# %  For this exercise, you will not need to change any code in this file,\n",
    "# %  or any other files other than those mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72dabc1e-8066-44b3-a2c8-622859aa21ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Visualizing Data ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading and Visualizing Data ...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a1f7b0a-501f-4477-92e9-0c0cd7f7bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Visualizing Data ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "\n",
    "# %% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  #% 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   #% 25 hidden units\n",
    "num_labels = 10          #% 10 labels, from 1 to 10   \n",
    "                          #% (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "# %% =========== Part 1: Loading and Visualizing Data =============\n",
    "# %  We start the exercise by first loading and visualizing the dataset. \n",
    "# %  You will be working with a dataset that contains handwritten digits.\n",
    "# %\n",
    "\n",
    "# % Load Training Data\n",
    "print('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "ex4data1 = scio.loadmat('ex4data1.mat')\n",
    "\n",
    "X = ex4data1['X']\n",
    "y = ex4data1['y']\n",
    "\n",
    "m = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1ebcc8-a6cb-40e7-a5e6-9c16b4622ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_7064/1033367891.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hugo\\AppData\\Local\\Temp/ipykernel_7064/1033367891.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    sel = sel(1:100);\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# % Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d6b5f60-4227-449a-9955-e40f1a9132f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10285, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% ================ Part 2: Loading Parameters ================\n",
    "# % In this part of the exercise, we load some pre-initialized \n",
    "# % neural network parameters.\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "print('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "# % Load the weights into variables Theta1 and Theta2\n",
    "\n",
    "ex4weights = scipy.io.loadmat('ex4weights.mat')\n",
    "\n",
    "Theta1 = ex4weights['Theta1']\n",
    "Theta2 = ex4weights['Theta2']\n",
    "\n",
    "# load('ex4weights.mat');\n",
    "\n",
    "# % Unroll parameters \n",
    "nn_params = np.append(Theta1 , Theta2)\n",
    "nn_params = nn_params.reshape(len(nn_params),1)\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6aede2ba-9d4e-4ae4-99a0-783c02702397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_lyr_sz, hidd_lyr_sz, num_lbls, X, y, lmbd):\n",
    "                                   \n",
    "    # %NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "    # %neural network which performs classification\n",
    "    # %   [J grad] = NNCOSTFUNCTON(nn_params, hidd_lyr_sz, num_lbls, ...\n",
    "    # %   X, y, lmbd) computes the cost and gradient of the neural network. The\n",
    "    # %   parameters for the neural network are \"unrolled\" into the vector\n",
    "    # %   nn_params and need to be converted back into the weight matrices. \n",
    "    # % \n",
    "    # %   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "    # %   partial derivatives of the neural network.\n",
    "    # %\n",
    "    # % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # % for our 2 layer neural network\n",
    "    \n",
    "    import numpy as np\n",
    "    from sigmoid import sigmoid\n",
    "    \n",
    "    Theta1 = np.reshape(nn_params[0:hidd_lyr_sz * (input_lyr_sz + 1)], (hidd_lyr_sz, input_lyr_sz + 1))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[hidd_lyr_sz * (input_lyr_sz + 1):], (num_lbls, hidd_lyr_sz + 1))\n",
    "\n",
    "    # % Setup some useful variables\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # % You need to return the following variables correctly \n",
    "\n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    # % Add ones to the X data matrix\n",
    "    \n",
    "    X = np.append(np.ones((m,1)), X, axis=1)\n",
    "    \n",
    "    # tic #QUE HAGO CON ESTO\n",
    "\n",
    "    # % ====================== YOUR CODE HERE ======================\n",
    "    # % Instructions: You should complete the code by working through the\n",
    "    # %               following parts.\n",
    "    # %\n",
    "    # % Part 1: Feedforward the neural network and return the cost in the\n",
    "    # %         variable J. After implementing Part 1, you can verify that your\n",
    "    # %         cost function computation is correct by verifying the cost\n",
    "    # %         computed in ex4.m\n",
    "    # %\n",
    "\n",
    "    yVec = np.zeros (( len(y), num_lbls))\n",
    "    y = y.reshape((len(y),1))\n",
    "\n",
    "    for k in range(0,num_lbls):\n",
    "        yVec[:,k] = np.where( y==k+1, 1,0 ).reshape((len(y)))\n",
    "        \n",
    "    # Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
    "    # a2 5000X25 a3 \n",
    "\n",
    "    z2 = np.dot(X, Theta1.T)\n",
    "\n",
    "    a2 = sigmoid( z2)\n",
    "    a2 = np.append(np.ones((a2.shape[0],1)), a2, axis=1) # a2 5000X26 \n",
    "    \n",
    "    #h_theta\n",
    "    z3 = np.dot(a2, Theta2.T)\n",
    "\n",
    "    a3 = sigmoid( z3); # a3 5000X10\n",
    "\n",
    "    J = (1- yVec) * np.log (1 - a3)\n",
    "\n",
    "    J = J + yVec * np.log (a3)\n",
    "\n",
    "    J = - 1/m * np.sum (np.sum( J ))\n",
    "\n",
    "    J = J + 0.5*lmbd/m*  np.sum( np.sum( Theta1[:,1:]**2)) \n",
    "\n",
    "    J = J + 0.5*lmbd/m* np.sum( np.sum( Theta2[:,1:]**2))\n",
    "\n",
    "    # % Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "    # %         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "    # %         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "    # %         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "    # %         that your implementation is correct by running checkNNGradients\n",
    "    # %\n",
    "    # %         Note: The vector y passed into the function is a vector of labels\n",
    "    # %               containing values from 1..K. You need to map this vector into a \n",
    "    # %               binary vector of 1's and 0's to be used with the neural network\n",
    "    # %               cost function.\n",
    "    # %\n",
    "    # %         Hint: We recommend implementing backpropagation using a for-loop\n",
    "    # %               over the training examples if you are implementing it for the \n",
    "    # %               first time.\n",
    "    # %\n",
    "\n",
    "    # Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
    "    # a2 5000X25 a3 5000X10\n",
    "\n",
    "    Delta1 = np.zeros( (hidd_lyr_sz, input_lyr_sz+1) )\n",
    "\n",
    "    Delta2 = np.zeros( (num_lbls, hidd_lyr_sz+1 ))\n",
    "\n",
    "    delta3 = a3 - yVec\n",
    "    \n",
    "    # delta2 = (  (Theta2' * delta3')' .* ( (a2.*( 1 - a2 )) )) ;\n",
    "#     ##           26X10      10X5000          5000X26\n",
    "    delta2 = (  np.dot(Theta2.T, delta3.T) ).T\n",
    "    \n",
    "    delta2 = delta2* ( a2*( 1 - a2))\n",
    "    \n",
    "    Delta2 = Delta2 + np.dot(delta3.T, a2)\n",
    "    \n",
    "    delta1 = np.dot(Theta1.T, delta2[:,1:].T).T\n",
    "    \n",
    "    delta1 =  delta1 * ( X* ( 1-X ) )\n",
    "    \n",
    "    delta1 = np.sum( delta1)\n",
    "    \n",
    "    Delta1 = Delta1 + np.dot(delta2.T, X)[1:,:];  \n",
    "    \n",
    "    Theta1_grad = np.zeros( (hidd_lyr_sz, input_lyr_sz+1) )\n",
    "\n",
    "    Theta2_grad = np.zeros( (num_lbls, hidd_lyr_sz+1 ))\n",
    "    \n",
    "    Theta1_grad[:, 1:] = Delta1[:, 1:] / m;\n",
    "\n",
    "    Theta2_grad[:, 1:] = Delta2[:, 1:] / m;\n",
    "\n",
    "#     % Part 3: Implement regularization with the cost function and gradients.\n",
    "#     %\n",
    "#     %         Hint: You can implement this around the code for\n",
    "#     %               backpropagation. That is, you can compute the gradients for\n",
    "#     %               the regularization separately and then add them to Theta1_grad\n",
    "#     %               and Theta2_grad from Part 2.\n",
    "#     %\n",
    "#     Theta1_grad(:,2:end) = Theta1_grad(:,2:end) + lmbd/m * Theta1(:,2:end);\n",
    "\n",
    "#     Theta2_grad(:,2:end) = Theta2_grad(:,2:end) + lmbd/m * Theta2(:,2:end);\n",
    "\n",
    "#     % -------------------------------------------------------------\n",
    "\n",
    "#     % =========================================================================\n",
    "\n",
    "#     % Unroll gradients\n",
    "#     grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "#     tiempos=toc;\n",
    "\n",
    "    # return [J, grad, tiempos]\n",
    "\n",
    "    return J\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb3a07-e7b7-4be0-b80a-55e4f44b7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ================ Part 3: Compute Cost (Feedforward) ================\n",
    "# %  To the neural network, you should first start by implementing the\n",
    "# %  feedforward part of the neural network that returns the cost only. You\n",
    "# %  should complete the code in nnCostFunction.m to return cost. After\n",
    "# %  implementing the feedforward to compute the cost, you can verify that\n",
    "# %  your implementation is correct by verifying that you get the same cost\n",
    "# %  as us for the fixed debugging parameters.\n",
    "# %\n",
    "# %  We suggest implementing the feedforward cost *without* regularization\n",
    "# %  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "# %  will get to implement the regularized cost.\n",
    "# %\n",
    "print('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "# % Weight regularization parameter (we set this to 0 here).\n",
    "lmbd = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48b59690-721b-42e2-b153-f8e359280197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.2876291651613189 \n",
      "(this value should be about 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from nnCostFunction import nnCostFunction\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd)\n",
    "\n",
    "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.287629)\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8a429-0e9a-46e0-8817-a29e9217c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken to calculate cost function and gradients using unvectorized function\n",
    "tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4644f581-0cfa-4c38-a91a-a12d1d1f012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "\n",
      "Cost at parameters (loaded from ex4weights): 0.38376985909092365 \n",
      "(this value should be about 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% =============== Part 4: Implement Regularization ===============\n",
    "# %  Once your cost function implementation is correct, you should now\n",
    "# %  continue to implement the regularization with the cost.\n",
    "# %\n",
    "\n",
    "print('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "# % Weight regularization parameter (we set this to 1 here).\n",
    "lmbd = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd);\n",
    "\n",
    "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.383770)\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d0d717d-dee6-4c26-ab25-86330e61b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ================ Part 5: Sigmoid Gradient  ================\n",
    "# %  Before you start implementing the neural network, you will first\n",
    "# %  implement the gradient for the sigmoid function. You should complete the\n",
    "# %  code in the sigmoidGradient.m file.\n",
    "# %\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "# %SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "# %evaluated at z\n",
    "# %   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "# %   evaluated at z. This should work regardless if z is a matrix or a\n",
    "# %   vector. In particular, if z is a vector or matrix, you should return\n",
    "# %   the gradient for each element.\n",
    "    \n",
    "    import numpy as np\n",
    "    from sigmoid import sigmoid\n",
    "    \n",
    "    z = np.array(z)\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "# % ====================== YOUR CODE HERE ======================\n",
    "# % Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "# %               each value of z (z can be a matrix, vector or scalar).\n",
    "\n",
    "    g = sigmoid(z) * (1 - sigmoid(z));\n",
    "    return g\n",
    "\n",
    "# % ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7c464553-8465-4a33-a905-55f189c4e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1, -0.5, 0, 0.5, 1])\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41fec75d-e666-414d-bb16-e45ee897ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ================ Part 6: Initializing Pameters ================\n",
    "# %  In this part of the exercise, you will be starting to implment a two\n",
    "# %  layer neural network that classifies digits. You will start by\n",
    "# %  implementing a function to initialize the weights of the neural network\n",
    "# %  (randInitializeWeights.m)\n",
    "\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    # %RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "    # %incoming connections and L_out outgoing connections\n",
    "    # %   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "    # %   of a layer with L_in incoming connections and L_out outgoing \n",
    "    # %   connections. \n",
    "    # %\n",
    "    # %   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "    # %   the first column of W handles the \"bias\" terms\n",
    "    # %\n",
    "\n",
    "    # % You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # % ====================== YOUR CODE HERE ======================\n",
    "    # % Instructions: Initialize W randomly so that we break the symmetry while\n",
    "    # %               training the neural network.\n",
    "    # %\n",
    "    # % Note: The first column of W corresponds to the parameters for the bias unit\n",
    "    # %\n",
    "\n",
    "    # % Randomly initialize the weights to small values\n",
    "    ##epsilon init = 0.12;\n",
    "    \n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in + L_out)\n",
    "\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    return W\n",
    "\n",
    "# % ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17518287-2f85-4d1b-bf9d-2ef0cf9f0a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# % Unroll parameters\n",
    "initial_nn_params = np.append(initial_Theta1 , initial_Theta2)\n",
    "initial_nn_params = initial_nn_params.reshape(len(nn_params),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b1499-af01-4889-aa76-9bcb93e1f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% =============== Part 7: Implement Backpropagation ===============\n",
    "# %  Once your cost matches up with ours, you should proceed to implement the\n",
    "# %  backpropagation algorithm for the neural network. You should add to the\n",
    "# %  code you've written in nnCostFunction.m to return the partial\n",
    "# %  derivatives of the parameters.\n",
    "# %\n",
    "print('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "# %  Check gradients by running checkNNGradients\n",
    "checkNNGradients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d18b5-4e04-4121-8b89-2d7d7dcf2221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe2213-ff6a-4f72-a93b-1cef8abc0c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cce1a2-4e0f-41db-8763-9941c5f532e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5615-bf76-4f18-a009-116085b7cb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "877f1a5d-0615-42ab-b81e-aea983096333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n",
      "\n",
      "Loading and Visualizing Data ...\n",
      "\n",
      "(5000, 10) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# %% ================ Part 2: Loading Parameters ================\n",
    "# % In this part of the exercise, we load some pre-initialized \n",
    "# % neural network parameters.\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "print('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "# % Load the weights into variables Theta1 and Theta2\n",
    "\n",
    "ex4weights = scipy.io.loadmat('ex4weights.mat')\n",
    "\n",
    "Theta1 = ex4weights['Theta1']\n",
    "Theta2 = ex4weights['Theta2']\n",
    "\n",
    "# load('ex4weights.mat');\n",
    "\n",
    "# % Unroll parameters \n",
    "nn_params = np.append(Theta1 , Theta2)\n",
    "nn_params = nn_params.reshape(len(nn_params),1)\n",
    "nn_params.shape\n",
    "\n",
    "\n",
    "\n",
    "nn_params, input_lyr_sz, hidd_lyr_sz, num_lbls, X, y, lmbd = nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbd\n",
    "\n",
    "import scipy.io as scio\n",
    "\n",
    "# %% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  #% 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   #% 25 hidden units\n",
    "num_labels = 10          #% 10 labels, from 1 to 10   \n",
    "                          #% (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "# %% =========== Part 1: Loading and Visualizing Data =============\n",
    "# %  We start the exercise by first loading and visualizing the dataset. \n",
    "# %  You will be working with a dataset that contains handwritten digits.\n",
    "# %\n",
    "\n",
    "# % Load Training Data\n",
    "print('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "ex4data1 = scio.loadmat('ex4data1.mat')\n",
    "\n",
    "X = ex4data1['X']\n",
    "y = ex4data1['y']\n",
    "\n",
    "m = X.shape[0]\n",
    "\n",
    "# %NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "# %neural network which performs classification\n",
    "# %   [J grad] = NNCOSTFUNCTON(nn_params, hidd_lyr_sz, num_lbls, ...\n",
    "# %   X, y, lmbd) computes the cost and gradient of the neural network. The\n",
    "# %   parameters for the neural network are \"unrolled\" into the vector\n",
    "# %   nn_params and need to be converted back into the weight matrices. \n",
    "# % \n",
    "# %   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "# %   partial derivatives of the neural network.\n",
    "# %\n",
    "# % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "# % for our 2 layer neural network\n",
    "\n",
    "import numpy as np\n",
    "    \n",
    "Theta1 = np.reshape(nn_params[0:hidd_lyr_sz * (input_lyr_sz + 1)], (hidd_lyr_sz, input_lyr_sz + 1))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[hidd_lyr_sz * (input_lyr_sz + 1):], (num_lbls, hidd_lyr_sz + 1))\n",
    "\n",
    "# % Setup some useful variables\n",
    "m = X.shape[0]\n",
    "\n",
    "# % You need to return the following variables correctly \n",
    "\n",
    "J = 0\n",
    "Theta1_grad = np.zeros(Theta1.shape)\n",
    "Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "# % Add ones to the X data matrix\n",
    "\n",
    "X = np.append(np.ones((m,1)), X, axis=1)\n",
    "\n",
    "# tic #QUE HAGO CON ESTO\n",
    "\n",
    "# % ====================== YOUR CODE HERE ======================\n",
    "# % Instructions: You should complete the code by working through the\n",
    "# %               following parts.\n",
    "# %\n",
    "# % Part 1: Feedforward the neural network and return the cost in the\n",
    "# %         variable J. After implementing Part 1, you can verify that your\n",
    "# %         cost function computation is correct by verifying the cost\n",
    "# %         computed in ex4.m\n",
    "# %\n",
    "\n",
    "# labelsVec = np.append(np.zeros((k,1)), 1)\n",
    "# labelsVec = np.append(labelsVec, np.zeros((num_lbls-k-1, 1)))\n",
    "# labelsVec = labelsVec.T\n",
    "\n",
    "import numpy.matlib\n",
    "\n",
    "# yVec = np.zeros (( len(y), 1, num_lbls+1))\n",
    "\n",
    "yVec = np.matlib.repmat(y, 1,num_lbls)\n",
    "\n",
    "for k in range(1,num_lbls+1):\n",
    "    yVec[:,k-1] = np.where( yVec[:,k-1]==k, 1,0 )\n",
    "\n",
    "    # idxs = np.find( y==k )\n",
    "\n",
    "    # WTFFF\n",
    "    # yVec[ idxs, : ] = repmat(labelsVec, length(idxs), 1)\n",
    "\n",
    "# Variables' sizes X 5000X401 yVec 5000x10, Theta1 25x401, Theta2 10X26\n",
    "# a2 5000X25 a3 \n",
    "\n",
    "z2 = np.dot(X, Theta1.T)\n",
    "\n",
    "from sigmoid import sigmoid\n",
    "\n",
    "a2 = sigmoid( z2)\n",
    "# [np.ones(m, 1) X]\n",
    "a2 = np.append(np.ones((a2.shape[0],1)), a2, axis=1)\n",
    "# a2 = [np.ones(a2.shape[0], 1) a2]; # a2 5000X26 \n",
    "\n",
    "#h_theta\n",
    "z3 = np.dot(a2, Theta2.T)\n",
    "\n",
    "# global a3\n",
    "\n",
    "a3 = sigmoid( z3); # a3 5000X10\n",
    "\n",
    "print(a3.shape, yVec.shape)\n",
    "\n",
    "J = (1- yVec) * np.log (1 - a3)\n",
    "\n",
    "J = J + yVec * np.log (a3)\n",
    "\n",
    "J = - 1/m * np.sum (np.sum( J ))\n",
    "\n",
    "J = J + 0.5*lmbd/m*  np.sum( np.sum( Theta1[:,1:]**2)) \n",
    "\n",
    "J = J + np.sum( np.sum( Theta2[:,1:]**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
